{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC4h1_lFQtDO"
      },
      "source": [
        "#Tech Challenge Fase 4 - Análise de video"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install deepface insightface tqdm onnxruntime"
      ],
      "metadata": {
        "id": "zz3WbitmQ2rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooXcEIYRQtDR"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from deepface import DeepFace\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "from insightface.app import FaceAnalysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBa1xTrKQtDS"
      },
      "outputs": [],
      "source": [
        "def generate_summary(emotion_counts, total_frames, output_path):\n",
        "    # Exibir Relatório no Console\n",
        "    print(\"RELATÓRIO:\")\n",
        "\n",
        "    # Exibir o total de frames analisados\n",
        "    print(f\"\\nTotal de frames analisados: {total_frames}\")\n",
        "\n",
        "    # Exibir o resumo das emoções detectadas\n",
        "    print(\"\\nResumo das emoções detectadas:\")\n",
        "    for emotion, count in emotion_counts.items():\n",
        "        print(f\"{emotion}: {count} ocorrências\")\n",
        "\n",
        "    # Exibir o número de anomalias detectadas (categorias 'Unknown')\n",
        "    anomalies = emotion_counts.get(\"Unknown\", 0)\n",
        "    print(f\"\\nNúmero de anomalias detectadas: {anomalies}\")\n",
        "\n",
        "    # Criar o arquivo de relatório\n",
        "    report_path = os.path.join(os.path.dirname(output_path), 'relatorio_analise.txt')\n",
        "\n",
        "    with open(report_path, 'w') as report_file:\n",
        "        # Escrever o total de frames analisados\n",
        "        report_file.write(f\"Total de frames analisados: {total_frames}\\n\")\n",
        "\n",
        "        # Escrever o resumo das emoções detectadas\n",
        "        report_file.write(\"\\nResumo das emoções detectadas:\\n\")\n",
        "        for emotion, count in emotion_counts.items():\n",
        "            report_file.write(f\"{emotion}: {count} ocorrências)\\n\")\n",
        "\n",
        "        # Escrever o número de anomalias detectadas\n",
        "        report_file.write(f\"\\nNúmero de anomalias detectadas: {anomalies}\\n\")\n",
        "\n",
        "    print(f\"\\nRelatório salvo em: {report_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfUgZpBHQtDS"
      },
      "outputs": [],
      "source": [
        "def detect_emotions(video_path, output_path):\n",
        "    # Capturar vídeo do arquivo especificado\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Verificar se o vídeo foi aberto corretamente\n",
        "    if not cap.isOpened():\n",
        "        print(\"Erro ao abrir o vídeo.\")\n",
        "        return\n",
        "\n",
        "    # Obter propriedades do vídeo\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Definir o codec e criar o objeto VideoWriter\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec para MP4\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    # Inicializar dicionário para contabilizar emoções\n",
        "    emotion_counts = {}\n",
        "\n",
        "    app = FaceAnalysis()\n",
        "    app.prepare(ctx_id=0)  # Use ctx_id=-1 for CPU\n",
        "\n",
        "    # Loop para processar cada frame do vídeo\n",
        "    for _ in tqdm(range(total_frames), desc=\"Processando vídeo\"):\n",
        "        # Ler um frame do vídeo\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        # Se não conseguiu ler o frame (final do vídeo), sair do loop\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        rgb_frame = frame[:, :, ::-1]\n",
        "\n",
        "        faces = app.get(rgb_frame)\n",
        "\n",
        "        # Manter o controle do número de faces detectadas\n",
        "        if 'prev_num_faces' not in locals():\n",
        "            prev_num_faces = 0\n",
        "\n",
        "        num_faces = len(faces)\n",
        "\n",
        "        faces_text = f\"Number of faces changed from {prev_num_faces} to {num_faces}\"\n",
        "        cv2.putText(frame, faces_text, (10, height - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
        "\n",
        "        if num_faces != prev_num_faces:\n",
        "            prev_num_faces = num_faces\n",
        "\n",
        "        for face in faces:\n",
        "            box = face.bbox.astype(int)\n",
        "\n",
        "            cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), (255, 0, 0), 2)\n",
        "\n",
        "            face_roi = rgb_frame[box[1]:box[3], box[0]:box[2]]\n",
        "\n",
        "            try:\n",
        "\n",
        "                analysis = DeepFace.analyze(face_roi, actions=['emotion'], enforce_detection=False, detector_backend='retinaface')\n",
        "\n",
        "                if isinstance(analysis, list) and len(analysis) > 0:\n",
        "                    dominant_emotion = analysis[0]['dominant_emotion']\n",
        "                else:\n",
        "                    dominant_emotion = \"Unknown\"\n",
        "\n",
        "            except Exception as e:\n",
        "                dominant_emotion = \"Error\"\n",
        "                print(f\"Error analyzing emotion: {e}\")\n",
        "\n",
        "            # Atualizar contador de emoções\n",
        "            if dominant_emotion not in emotion_counts:\n",
        "                emotion_counts[dominant_emotion] = 0\n",
        "            emotion_counts[dominant_emotion] += 1\n",
        "\n",
        "            cv2.putText(frame, dominant_emotion, (box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n",
        "\n",
        "        # Escrever o frame processado no arquivo de vídeo de saída\n",
        "        out.write(frame)\n",
        "\n",
        "    # Chamar a função para gerar o relatório\n",
        "    generate_summary(emotion_counts, total_frames, output_path)\n",
        "\n",
        "    # Liberar a captura de vídeo e fechar todas as janelas\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldbHa00NQtDT",
        "outputId": "f2596dbd-dcc3-4215-de99-2795a02d8396",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 977
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download_path: /root/.insightface/models/buffalo_l\n",
            "Downloading /root/.insightface/models/buffalo_l.zip from https://github.com/deepinsight/insightface/releases/download/v0.7/buffalo_l.zip...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 281857/281857 [00:03<00:00, 83249.35KB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
            "set det-size: (640, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessando vídeo:   0%|          | 0/3326 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24-11-20 20:37:35 - facial_expression_model_weights.h5 will be downloaded...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://github.com/serengil/deepface_models/releases/download/v1.0/facial_expression_model_weights.h5\n",
            "To: /root/.deepface/weights/facial_expression_model_weights.h5\n",
            "\n",
            "100%|██████████| 5.98M/5.98M [00:00<00:00, 400MB/s]\n",
            "Processando vídeo: 100%|██████████| 3326/3326 [19:48<00:00,  2.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RELATÓRIO:\n",
            "\n",
            "Total de frames analisados: 3326\n",
            "\n",
            "Resumo das emoções detectadas:\n",
            "fear: 1062 ocorrências\n",
            "angry: 386 ocorrências\n",
            "neutral: 509 ocorrências\n",
            "sad: 1376 ocorrências\n",
            "Unknown: 312 ocorrências\n",
            "surprise: 195 ocorrências\n",
            "happy: 856 ocorrências\n",
            "disgust: 1 ocorrências\n",
            "\n",
            "Número de anomalias detectadas: 312\n",
            "\n",
            "Relatório salvo em: /content/relatorio_analise.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "OpenCV(4.10.0) /io/opencv/modules/highgui/src/window.cpp:1295: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvDestroyAllWindows'\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-e8eed56b1935>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Chamar a função para detectar emoções no vídeo e salvar o vídeo processado\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdetect_emotions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_video_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_video_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-36d31b036d47>\u001b[0m in \u001b[0;36mdetect_emotions\u001b[0;34m(video_path, output_path)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) /io/opencv/modules/highgui/src/window.cpp:1295: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvDestroyAllWindows'\n"
          ]
        }
      ],
      "source": [
        "# Caminho para o arquivo de vídeo na mesma pasta do script\n",
        "script_dir = os.path.dirname(os.path.abspath('__file__'))\n",
        "input_video_path = os.path.join(script_dir, 'video.mp4')\n",
        "output_video_path = os.path.join(script_dir, 'output_video_12.mp4')  # Nome do vídeo de saída\n",
        "\n",
        "# Chamar a função para detectar emoções no vídeo e salvar o vídeo processado\n",
        "detect_emotions(input_video_path, output_video_path)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}